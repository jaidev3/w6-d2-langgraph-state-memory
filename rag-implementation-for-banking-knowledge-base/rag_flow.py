# user query
# get the relevant chunks from the pinecone index
# pass the chunks to the LLM
# return the answer


# from langchain_pinecone import PineconeVectorStore
# from langchain_openai import ChatOpenAI
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.chains import RetrievalQA
# from langchain_core.retrievers import ContextualCompressionRetriever, LLMChainRetriever, SemanticRetriever



def get_relevant_chunks(user_query):
    # get the relevant chunks from the pinecone index
    pass





